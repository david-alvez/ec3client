description hadoop (
    kind = 'framework' and
    short = 'Open-source software for reliable, scalable, distributed computing' and
    content = 'The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.
Webpage: http://hadoop.apache.org/'
)


network private ()

system front (
    cpu.count>=1 and
    memory.size>=4192m and
    net_interface.0.connection = 'private' and
    net_interface.1.connection = 'public' and
    disk.0.os.name = 'linux' and
    disk.0.applications contains (name='ansible.modules.indigo-dc.hadoop')
)

system wn (
    net_interface.0.connection='private'
)



configure createbigseahdfs (
@begin
---
    - command: /opt/hadoop/bin/hdfs dfs -mkdir -p /users/
    - command: /opt/hadoop/bin/hdfs dfs -mkdir -p /log/
    - command: /opt/hadoop/bin/hdfs dfs -mkdir -p /log/history
    - command: /opt/hadoop/bin/hdfs dfs -chmod go+w /log
    - command: /opt/hadoop/bin/hdfs dfs -chmod go+w /log/history

    - name: configure environment variables
      copy:
        dest: /etc/profile.d/hadoop.sh
        content: |
            export PATH="$PATH:/opt/hadoop/bin"
            export CLASSPATH="/opt/hadoop-2.7.4/etc/hadoop:/opt/hadoop-2.7.4/share/hadoop/common/lib/*:/opt/hadoop-2.7.4/share/hadoop/common/*:/opt/hadoop-2.7.4/share/hadoop/hdfs:/opt/hadoop-2.7.4/share/hadoop/hdfs/lib/*:/opt/hadoop-2.7.4/share/hadoop/hdfs/*:/opt/hadoop-2.7.4/share/hadoop/yarn/lib/*:/opt/hadoop-2.7.4/share/hadoop/yarn/*:/opt/hadoop-2.7.4/share/hadoop/mapreduce/lib/*:/opt/hadoop-2.7.4/share/hadoop/mapreduce/*:/contrib/capacity-scheduler/*.jar"

    - name: create HDFS homes
      command: /opt/hadoop/bin/hdfs dfs -mkdir -p /users/{{ item.name }}
      with_items:
	  - { name: 'user01' }
      - { name: 'user02' }
      - { name: 'user03' }
      - { name: 'user04' }
      - { name: 'user05' }
      - { name: 'user07' }
      - { name: 'user08' }
      - { name: 'user09' }
      - { name: 'user10' }

    - name: create HDFS homes
      command: /opt/hadoop/bin/hdfs dfs -chown {{ item.name }}:{{ item.name }} /users/{{ item.name }}
      with_items:
	  - { name: 'user01' }
      - { name: 'user02' }
      - { name: 'user03' }
      - { name: 'user04' }
      - { name: 'user05' }
      - { name: 'user07' }
      - { name: 'user08' }
      - { name: 'user09' }
      - { name: 'user10' }
@end
)



configure front (
@begin
---
  - ec3_prio: -5
    roles:
#      - { role: 'indigo-dc.hadoop', hdfs_props: { "dfs.datanode.data.dir": "file:///opt/dfs/data" }, hadoop_master: "{{ hostvars[groups['front'][0]]['IM_NODE_PRIVATE_IP'] }}", hadoop_type_of_node: 'master'}
      - { role: 'indigo-dc.hadoop', hadoop_version: "2.7.4", hadoop_mirrors: [ "ftp://ftpgrycap.i3m.upv.es/public/eubrabigsea/data/" ], hdfs_props: { "dfs.datanode.data.dir": "file:///opt/dfs/data" }, hadoop_master: "{{ hostvars[groups['front'][0]]['IM_NODE_PRIVATE_IP'] }}", hadoop_type_of_node: 'master'}

    post_tasks:
      - include: createbigseahdfs.yml


@end
)

configure datanode (
@begin
---
  - ec3_prio: -5
    vars:
      TEMPLATES:
        ec3_jpath: /system/front/ec3_templates

    roles:
#    - { role: 'indigo-dc.hadoop', hdfs_props: { "dfs.datanode.data.dir": "file:///opt/dfs/data" }, hadoop_master: "{{ hostvars[groups['front'][0]]['IM_NODE_PRIVATE_IP'] }}" }
    - { role: 'indigo-dc.hadoop', hadoop_mirrors: [ "ftp://ftpgrycap.i3m.upv.es/public/eubrabigsea/data/" ], hdfs_props: { "dfs.datanode.data.dir": "file:///opt/dfs/data" }, hadoop_master: "{{ hostvars[groups['front'][0]]['IM_NODE_PRIVATE_IP'] }}" }
@end
)

#deploy datanode 3
